{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f878d1-375f-4634-91cd-2b0aa53e3f76",
   "metadata": {},
   "source": [
    "# Building a Chatbot with NLP and GRU model and attention mechanism\n",
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70749118-a8e1-4a53-8cb8-cfa005b13673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable = True)\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b697d58-2142-48bb-8344-ae61baf43787",
   "metadata": {},
   "source": [
    "### Importing preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e999f2-0958-40a6-b5fb-16f514a1e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./preprocessed_data/questions.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    question_corpus = tokenizer_from_json(json_data)\n",
    "    f.close()\n",
    "\n",
    "with open('./preprocessed_data/answers.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    answer_corpus = tokenizer_from_json(json_data)\n",
    "    f.close()\n",
    "\n",
    "npzfile = np.load('./preprocessed_data/data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7dd5d-fd81-467e-a7ab-62941a561da5",
   "metadata": {},
   "source": [
    "### Creating the Encoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaf31f6-e611-4f91-a82d-18c299cc4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences = True, return_state = True,\n",
    "                                       recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # Embed input words  \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Pass the embedded word vectors into LSTM and return all outputs\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b8985-21df-47a3-86ce-0c1e77f1f3df",
   "metadata": {},
   "source": [
    "### Creating the Bahdanau Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e730853-4072-4ae6-bc25-d0dfb46cd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.R = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # Calculating Alignment Scores\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        score = self.R(score)\n",
    "        \n",
    "        # Softmaxing alignment scores to get Attention weights\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # Multiplying the Attention weights with encoder outputs to get the context vector\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdb199-86a9-4279-b51e-9f2ffd1255fd",
   "metadata": {},
   "source": [
    "### Creating the Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539d020b-8c92-4f72-a086-f981cf6902e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences = True, return_state = True,\n",
    "                                       recurrent_initializer = 'glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c06893-584b-4f01-8da1-379f3e0c1241",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbad2503-4484-4a17-8aad-318977618379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretically, vocabulary size should be len(question_corpus.word_index) + 1. \n",
    "# However, it seems like the 'num_words' didn't filter the tokenizer. so we assign the number manually\n",
    "vocab_size = 1001\n",
    "embedding_size = 128\n",
    "n_unit = 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c93251ff-cf5f-4bcb-98b1-936fa876c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_size, n_unit, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630486bf-0bbf-4930-b8e0-c036fd021391",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_size, embedding_size, n_unit, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fc28e-a6ef-4339-aaab-95c319e8389d",
   "metadata": {},
   "source": [
    "### Defining the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb44989-000e-41aa-8a43-5b83d188a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86d383-f5ae-4491-a600-9102715ffaeb",
   "metadata": {},
   "source": [
    "### Defining the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9058875-d6d0-4a76-b27b-4268ecdc36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([answer_corpus.word_index['bos']] * batch_size, 1)\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e77e2f-b9b2-4331-b423-4419e9be7392",
   "metadata": {},
   "source": [
    "### Defining the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a486ff5-343c-48fa-9614-43ab20d02490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([answer_corpus.word_index['bos']] * batch_size, 1)\n",
    "    \n",
    "    for t in range(1, targ.shape[1]):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b718533-a9e8-4191-ba7c-61b3b8e11e9d",
   "metadata": {},
   "source": [
    "### Defining the parameter to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80f803b2-5258-4f85-923e-10254b40b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118190\n"
     ]
    }
   ],
   "source": [
    "train_valid_split = int(len(npzfile['arr_0']) * 0.8)\n",
    "print(train_valid_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761c2b9-d652-478c-88a2-efd42eb405fa",
   "metadata": {},
   "source": [
    "### Getting the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7578571e-f210-449b-93f5-8e25700fbc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question = npzfile['arr_0'][:train_valid_split]\n",
    "input_answers = npzfile['arr_1'][:train_valid_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ac39a-b890-4fb5-a667-dc4625b0005b",
   "metadata": {},
   "source": [
    "### Getting the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51b73550-f616-4465-9e2c-e45b83c2e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_questions = npzfile['arr_0'][train_valid_split:]\n",
    "valid_answers = npzfile['arr_1'][train_valid_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a038b3-91bb-4fad-bf1a-3f4af787aa2a",
   "metadata": {},
   "source": [
    "### Creating tensorflow dataset pipeline for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47dab72a-76e8-4a4a-b26e-4cf7b6cba193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "buffer_size1 = len(input_question)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((input_question, input_answers)).shuffle(buffer_size1)\n",
    "dataset_train = dataset_train.batch(batch_size, drop_remainder = True)\n",
    "\n",
    "# Validation set\n",
    "buffer_size2 = len(valid_questions)\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((valid_questions, valid_answers)).shuffle(buffer_size2)\n",
    "dataset_valid = dataset_valid.batch(batch_size, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe14417-a91f-4dff-aa2c-0cc9bedf0348",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "480128b3-128c-4915-a391-6499f7c0bdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1     Loss: 0.995     Valid_Loss: 0.956\n",
      "Time taken for 1 epoch: 2034.1076443195343 sec\n",
      "\n",
      "Epoch: 2     Loss: 0.922     Valid_Loss: 0.930\n",
      "Time taken for 1 epoch: 1852.464063167572 sec\n",
      "\n",
      "Epoch: 3     Loss: 0.900     Valid_Loss: 0.917\n",
      "Time taken for 1 epoch: 1855.9490611553192 sec\n",
      "\n",
      "Epoch: 4     Loss: 0.885     Valid_Loss: 0.913\n",
      "Time taken for 1 epoch: 1851.9534990787506 sec\n",
      "\n",
      "Epoch: 5     Loss: 0.873     Valid_Loss: 0.908\n",
      "Time taken for 1 epoch: 1856.7145359516144 sec\n",
      "\n",
      "Epoch: 6     Loss: 0.863     Valid_Loss: 0.906\n",
      "Time taken for 1 epoch: 1870.2493431568146 sec\n",
      "\n",
      "Epoch: 7     Loss: 0.853     Valid_Loss: 0.909\n",
      "Time taken for 1 epoch: 1857.4066376686096 sec\n",
      "\n",
      "Epoch: 8     Loss: 0.844     Valid_Loss: 0.911\n",
      "Time taken for 1 epoch: 1857.5119829177856 sec\n",
      "\n",
      "Epoch: 9     Loss: 0.834     Valid_Loss: 0.914\n",
      "Time taken for 1 epoch: 1847.0697631835938 sec\n",
      "\n",
      "Epoch: 10     Loss: 0.824     Valid_Loss: 0.917\n",
      "Time taken for 1 epoch: 1851.0322816371918 sec\n",
      "\n",
      "Epoch: 11     Loss: 0.815     Valid_Loss: 0.924\n",
      "Time taken for 1 epoch: 1848.1049692630768 sec\n",
      "\n",
      "Epoch: 12     Loss: 0.806     Valid_Loss: 0.930\n",
      "Time taken for 1 epoch: 1852.1914749145508 sec\n",
      "\n",
      "Epoch: 13     Loss: 0.797     Valid_Loss: 0.935\n",
      "Time taken for 1 epoch: 1858.2791056632996 sec\n",
      "\n",
      "Epoch: 14     Loss: 0.789     Valid_Loss: 0.942\n",
      "Time taken for 1 epoch: 1855.577615737915 sec\n",
      "\n",
      "Epoch: 15     Loss: 0.780     Valid_Loss: 0.947\n",
      "Time taken for 1 epoch: 1855.1691300868988 sec\n",
      "\n",
      "Overall time taken: 466.7296851317088 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "trainstep_epoch = len(input_question)//batch_size\n",
    "validstep_epoch = len(valid_questions)//batch_size\n",
    "overall_time = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    valid_loss = 0\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for (batch, (input_question, input_answers)) in enumerate(dataset_train.take(trainstep_epoch)):\n",
    "        batch_loss = train_step(input_question, input_answers, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    for (batch, (valid_questions, valid_answers)) in enumerate(dataset_valid.take(validstep_epoch)):\n",
    "        valid_batch_loss = validation_loss(valid_questions, valid_answers, enc_hidden)\n",
    "        valid_loss += valid_batch_loss\n",
    "        \n",
    "    print('Epoch: {}     Loss: {:.3f}     Valid_Loss: {:.3f}'.format(epoch + 1, total_loss/trainstep_epoch, valid_loss/validstep_epoch))\n",
    "    \n",
    "    stop = time.time()\n",
    "    timetaken = stop - start\n",
    "    print('Time taken for 1 epoch: {} sec\\n'.format(timetaken))\n",
    "    \n",
    "    overall_time += timetaken\n",
    "    \n",
    "print('Overall time taken: {} min\\n'.format(overall_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f272b6-a6a4-4948-aba7-9548670f3093",
   "metadata": {},
   "source": [
    "### Saving parameters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2615314-1500-4825-9830-3bc28a9407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights('./trained_model/attention_encoder_test1000.h5')\n",
    "decoder.save_weights('./trained_model/attention_decoder_test1000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319c05f-a5ab-469a-a344-c29eef181e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
